{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing libraries**"
      ],
      "metadata": {
        "id": "LPptOg1rOgZk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ugeLg0ddCpcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b20bf49-1994-411b-c403-88984e46eb87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 2s (2,659 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123599 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20240706)\n",
            "Requirement already satisfied: tesserocr in /usr/local/lib/python3.10/dist-packages (2.7.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.5.15)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: poppler-utils in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.10/dist-packages (from poppler-utils) (8.1.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package tesseract-oc\n",
            "tesseract 4.1.1\n",
            " leptonica-1.82.0\n",
            "  libgif 5.1.9 : libjpeg 8d (libjpeg-turbo 2.1.1) : libpng 1.6.37 : libtiff 4.3.0 : zlib 1.2.11 : libwebp 1.2.2 : libopenjp2 2.4.0\n",
            " Found AVX2\n",
            " Found AVX\n",
            " Found FMA\n",
            " Found SSE\n",
            " Found libarchive 3.6.0 zlib/1.2.11 liblzma/5.2.5 bz2lib/1.0.8 liblz4/1.9.3 libzstd/1.4.8\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries for OCR and NLP\n",
        "!pip install pytesseract Pillow pdfminer.six tesserocr regex spacy poppler-utils\n",
        "!pip install transformers torch\n",
        "# Install Tesseract OCR engine\n",
        "!sudo apt-get install tesseract-ocr\n",
        "# Verify Tesseract installation by checking its version\n",
        "!tesseract --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Extraction from different formats**\n",
        "### This code performs resume data extraction from PDFs or images using OCR, text processing, NLP techniques and JSON Schema. It first extracts text from a PDF or image, cleans the text, and uses regex and NLP to identify key sections like personal information, work experience, education, skills, and certifications. The extracted information is then structured into a JSON format, which can be further processed or analyzed."
      ],
      "metadata": {
        "id": "NFu1dlRuOvjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json  ## json v imp\n",
        "import spacy  ## NLP library / NLTK\n",
        "import pytesseract   ## OCR\n",
        "from pdfminer.high_level import extract_text\n",
        "from PIL import Image ## image\n",
        "\n",
        "# Load the pre-trained NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the function\n",
        "def extract_resume_data(file_path):\n",
        "    # Helper function to extract text from PDF or image\n",
        "    def ocr_text_extraction(file_path):\n",
        "        if file_path.endswith('.pdf'):\n",
        "            text = extract_text(file_path)\n",
        "            return text\n",
        "        elif file_path.endswith(('.png', '.jpg', '.jpeg')):\n",
        "            # Apply OCR directly on image\n",
        "            img = Image.open(file_path)\n",
        "            return pytesseract.image_to_string(img)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Use PDF or image.\")\n",
        "\n",
        "    # Helper function for text preprocessing\n",
        "    def preprocess_text(text):\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "        text = text.strip()  # Remove leading/trailing spaces\n",
        "        return text\n",
        "\n",
        "    # Helper function to extract sections using regex\n",
        "    def extract_section(text, keyword):\n",
        "        pattern = rf'({keyword}.*?)(\\n\\n|\\Z)'\n",
        "        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        return None\n",
        "\n",
        "    # Extract text using OCR\n",
        "    raw_text = ocr_text_extraction(file_path)\n",
        "\n",
        "    # Preprocess the text\n",
        "    clean_text = preprocess_text(raw_text)\n",
        "\n",
        "    # Use regex and NLP to identify sections\n",
        "    personal_info = extract_section(clean_text, 'Personal Information|Contact Information')\n",
        "    work_experience = extract_section(clean_text, 'Work Experience|Professional Experience|Employment')\n",
        "    education = extract_section(clean_text, 'Education|Academic Background|Qualifications')\n",
        "    skills = extract_section(clean_text, 'Skills|Technical Skills|Core Competencies')\n",
        "    certifications = extract_section(clean_text, 'Certifications|Licenses|Accreditations')\n",
        "\n",
        "    # If NLP is needed for further classification or entity extraction\n",
        "    doc = nlp(clean_text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # Create structured JSON output\n",
        "    resume_data = {\n",
        "        \"personal_information\": personal_info if personal_info else \"N/A\",\n",
        "        \"work_experience\": work_experience if work_experience else \"N/A\",\n",
        "        \"education\": education if education else \"N/A\",\n",
        "        \"skills\": skills if skills else \"N/A\",\n",
        "        \"certifications\": certifications if certifications else \"N/A\",\n",
        "        \"entities\": entities  # Captured using SpaCy\n",
        "    }\n",
        "\n",
        "    # Convert to JSON format\n",
        "    json_output = json.dumps(resume_data, indent=4)\n",
        "    return json_output, clean_text\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Resume in pdf format.pdf'\n",
        "resume_json,extracted_text = extract_resume_data(file_path)\n",
        "print(resume_json)\n"
      ],
      "metadata": {
        "id": "he-S4Fa4CxWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa464b9-5047-4f48-b38f-428cadbf143f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"personal_information\": \"N/A\",\n",
            "    \"work_experience\": \"employment benefit options. Arranged hospital-wide guest speakers symposia to educate management about new employment laws and workplace confidence and morale building techniques. Administrative tasks. Skills Type 96WPM \\u2022 Proficient with Workday \\u2022 Team player \\u2022 Excellent time management skills \\u2022 Conflict Management \\u2022 Public Speaking \\u2022 Data analytics Education MAY 2012 Bachelor of Arts Human Resources Management/Beachy University, Sunny, Florida Activities Literature \\u2022 Environmental conservation \\u2022 Art \\u2022 Yoga \\u2022 Skiing \\u2022 Travel\",\n",
            "    \"education\": \"N/A\",\n",
            "    \"skills\": \"N/A\",\n",
            "    \"certifications\": \"N/A\",\n",
            "    \"entities\": [\n",
            "        [\n",
            "            \"Janna Gardner\",\n",
            "            \"PERSON\"\n",
            "        ],\n",
            "        [\n",
            "            \"4567\",\n",
            "            \"DATE\"\n",
            "        ],\n",
            "        [\n",
            "            \"Chico\",\n",
            "            \"GPE\"\n",
            "        ],\n",
            "        [\n",
            "            \"Illinois 98052\",\n",
            "            \"ORG\"\n",
            "        ],\n",
            "        [\n",
            "            \"716\",\n",
            "            \"CARDINAL\"\n",
            "        ],\n",
            "        [\n",
            "            \"6+ years\",\n",
            "            \"DATE\"\n",
            "        ],\n",
            "        [\n",
            "            \"2014\",\n",
            "            \"DATE\"\n",
            "        ],\n",
            "        [\n",
            "            \"Lamna Healthcare Company\",\n",
            "            \"ORG\"\n",
            "        ],\n",
            "        [\n",
            "            \"Chico\",\n",
            "            \"GPE\"\n",
            "        ],\n",
            "        [\n",
            "            \"Illinois Review\",\n",
            "            \"ORG\"\n",
            "        ],\n",
            "        [\n",
            "            \"OSHA\",\n",
            "            \"ORG\"\n",
            "        ],\n",
            "        [\n",
            "            \"over 10%\",\n",
            "            \"PERCENT\"\n",
            "        ],\n",
            "        [\n",
            "            \"90%\",\n",
            "            \"PERCENT\"\n",
            "        ],\n",
            "        [\n",
            "            \"2-year\",\n",
            "            \"DATE\"\n",
            "        ],\n",
            "        [\n",
            "            \"year-over-year\",\n",
            "            \"DATE\"\n",
            "        ],\n",
            "        [\n",
            "            \"14%\",\n",
            "            \"PERCENT\"\n",
            "        ],\n",
            "        [\n",
            "            \"JUNE 2012\",\n",
            "            \"DATE\"\n",
            "        ],\n",
            "        [\n",
            "            \"2014\",\n",
            "            \"DATE\"\n",
            "        ],\n",
            "        [\n",
            "            \"Boomtown\",\n",
            "            \"GPE\"\n",
            "        ],\n",
            "        [\n",
            "            \"Ohio Assisted\",\n",
            "            \"ORG\"\n",
            "        ],\n",
            "        [\n",
            "            \"symposia\",\n",
            "            \"GPE\"\n",
            "        ],\n",
            "        [\n",
            "            \"Skills Type 96WPM \\u2022 Proficient\",\n",
            "            \"ORG\"\n",
            "        ],\n",
            "        [\n",
            "            \"Workday \\u2022 Team\",\n",
            "            \"ORG\"\n",
            "        ],\n",
            "        [\n",
            "            \"\\u2022 Conflict Management \\u2022\",\n",
            "            \"ORG\"\n",
            "        ],\n",
            "        [\n",
            "            \"MAY 2012\",\n",
            "            \"DATE\"\n",
            "        ],\n",
            "        [\n",
            "            \"Bachelor of Arts Human Resources Management/Beachy University\",\n",
            "            \"ORG\"\n",
            "        ],\n",
            "        [\n",
            "            \"Sunny\",\n",
            "            \"GPE\"\n",
            "        ],\n",
            "        [\n",
            "            \"Florida Activities Literature \\u2022 Environmental\",\n",
            "            \"ORG\"\n",
            "        ]\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extracted Text**"
      ],
      "metadata": {
        "id": "yPmr2pxpPlAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_text"
      ],
      "metadata": {
        "id": "sIflJWHNEMED",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "92055e57-b352-4868-d353-c298c6abec5e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Janna Gardner 4567 Main Street, Chico, Illinois 98052 (716) 555-0100 j.gardner@live.com www.linkedin.com/in/j.gardner Human Resources Generalist with 6+ years of experience assisting with and fulfilling organization staffing needs and requirements. A proven track record of using my excellent personal, communication and organization skills to lead and improve HR departments, recruit excellent personnel, and improve department efficiencies. Team player with excellent communication skills, high quality of work, driven and highly self-motivated. Strong negotiating skills and business acumen and able to work independently. Experience 2014 – PRESENT Human Resources Generalist/Lamna Healthcare Company, Chico, Illinois Review, update, and revise company hiring practices, vacation, and other human resources policies to ensure compliance with OSHA and all local, state, and federal labor regulations. By creating and maintaining a positive and responsive work environment, raised employee retention rates by over 10% to achieve a greater than 90% employee retention over a 2-year period. Developed recruitment programs to successfully increase minority recruitment and meet affirmative action requirements. Lead development team to build and deploy a dedicated a recruitment website which reduced year-over-year recruitment costs by 14%. JUNE 2012 – AUGUST 2014 Human Resources Intern/ZDF Hospital, Boomtown, Ohio Assisted in recruitment outreach to prospective employees. Organized and conducted several seminars for hospital employees to educate and update them regarding available employment benefit options. Arranged hospital-wide guest speakers symposia to educate management about new employment laws and workplace confidence and morale building techniques. Administrative tasks. Skills Type 96WPM • Proficient with Workday • Team player • Excellent time management skills • Conflict Management • Public Speaking • Data analytics Education MAY 2012 Bachelor of Arts Human Resources Management/Beachy University, Sunny, Florida Activities Literature • Environmental conservation • Art • Yoga • Skiing • Travel'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load a pre-trained BERT model for sequence classification.**\n",
        "### It loads the BERT tokenizer and model, specifically the 'bert-base-uncased' version, which can be replaced with a fine-tuned model if available. A classification pipeline is created to easily classify text input using the pre-trained BERT model and tokenizer."
      ],
      "metadata": {
        "id": "u3m7YdnbP2EB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'  # Replace with your fine-tuned model name if available\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create a pipeline for classification\n",
        "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "D4uhmxFgC3_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769944f9-0324-4c3e-cef3-a54fee817cb1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Classifying Sections**\n",
        "### This function, `classify_sections`, takes a block of text and a classifier to categorize the text into predefined sections. The text is split into individual sentences, and each sentence is classified using the provided classifier. Based on the classification label (e.g., PERSONAL_INFORMATION, WORK_EXPERIENCE), sentences are added to the respective section in a dictionary. The function returns a dictionary with categorized sentences for each section, such as personal information, work experience, education, skills, and certifications."
      ],
      "metadata": {
        "id": "CZHBXyf8QVli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sections(text, classifier):\n",
        "    sentences = text.split('\\n')  # Simplistic sentence splitting; can be customized as needed\n",
        "\n",
        "    classified_data = {\n",
        "        \"personal_information\": [],\n",
        "        \"work_experience\": [],\n",
        "        \"education\": [],\n",
        "        \"skills\": [],\n",
        "        \"certifications\": []\n",
        "    }\n",
        "\n",
        "    for sentence in sentences:\n",
        "        result = classifier(sentence)\n",
        "        label = result[0]['label']\n",
        "\n",
        "        # Add sentence to the corresponding section\n",
        "        if label == \"PERSONAL_INFORMATION\":\n",
        "            classified_data[\"personal_information\"].append(sentence)\n",
        "        elif label == \"WORK_EXPERIENCE\":\n",
        "            classified_data[\"work_experience\"].append(sentence)\n",
        "        elif label == \"EDUCATION\":\n",
        "            classified_data[\"education\"].append(sentence)\n",
        "        elif label == \"SKILLS\":\n",
        "            classified_data[\"skills\"].append(sentence)\n",
        "        elif label == \"CERTIFICATIONS\":\n",
        "            classified_data[\"certifications\"].append(sentence)\n",
        "\n",
        "    return classified_data\n"
      ],
      "metadata": {
        "id": "zAxrChRzC79F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing of transformers**\n",
        "### This code snippet installs the `transformers` library, essential for using pre-trained models like BERT. It imports necessary modules from `torch` and `transformers` to load a pre-trained BERT model and tokenizer for sequence classification. A text classification pipeline is created using BERT, enabling sentence-level classification of resume sections. The `classify_sections` function splits the input text into sentences, truncates any sentence longer than 510 tokens to fit within BERT's token limit, and classifies each sentence into one of five categories: personal information, work experience, education, skills, or certifications. The classified sentences are then organized into their respective categories."
      ],
      "metadata": {
        "id": "VBS7DToDR6NS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'  # Replace with your fine-tuned model name if available\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create a pipeline for classification\n",
        "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
        "\n",
        "def classify_sections(text, classifier):\n",
        "    sentences = text.split('\\n')  # Simplistic sentence splitting; customize as needed\n",
        "\n",
        "    classified_data = {\n",
        "        \"personal_information\": [],\n",
        "        \"work_experience\": [],\n",
        "        \"education\": [],\n",
        "        \"skills\": [],\n",
        "        \"certifications\": []\n",
        "    }\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Truncate the sentence if it exceeds the maximum length\n",
        "        tokens = tokenizer.tokenize(sentence)\n",
        "        if len(tokens) > 510:  # Account for [CLS] and [SEP] tokens\n",
        "            tokens = tokens[:510]\n",
        "        sentence = tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "        result = classifier(sentence)\n",
        "        label = result[0]['label']\n",
        "\n",
        "        # Add sentence to the corresponding section\n",
        "        if label == \"PERSONAL_INFORMATION\":\n",
        "            classified_data[\"personal_information\"].append(sentence)\n",
        "        elif label == \"WORK_EXPERIENCE\":\n",
        "            classified_data[\"work_experience\"].append(sentence)\n",
        "        elif label == \"EDUCATION\":\n",
        "            classified_data[\"education\"].append(sentence)\n",
        "        elif label == \"SKILLS\":\n",
        "            classified_data[\"skills\"].append(sentence)\n",
        "        elif label == \"CERTIFICATIONS\":\n",
        "            classified_data[\"certifications\"].append(sentence)\n",
        "\n",
        "    return classified_data"
      ],
      "metadata": {
        "id": "uUnDhTZdDAN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "149d8099-2cf2-4c3b-a5cc-ae338361bd8c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Classification**"
      ],
      "metadata": {
        "id": "AeJBLY9VgedB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classified_data = classify_sections(extracted_text, classifier)\n",
        "classified_data"
      ],
      "metadata": {
        "id": "tY_emxIzDFg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce1c158-48ea-4a67-825f-7d119c7b5981"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'personal_information': [],\n",
              " 'work_experience': [],\n",
              " 'education': [],\n",
              " 'skills': [],\n",
              " 'certifications': []}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}